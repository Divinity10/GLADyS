# Learning & Planning Validation Options

**Date**: 2026-01-24
**Topic**: Next steps for validating the Learning & Planning subsystem

## Context
The PoC has successfully validated isolated components of the Learning architecture:
1.  **Confidence Updates** (`test_td_learning.py`): Proves the math for updating heuristic weights works.
2.  **Pattern Extraction** (`test_heuristic_flow.py`): Proves the LLM can extract a JSON pattern from a scenario given feedback.
3.  **Fast Path Matching** (`test_killer_feature.py`): Proves the Rust service can match an existing heuristic and skip the LLM.

However, we have not validated that these components feed into each other correctly at runtime. The "Learning Loop" remains theoretically connected but practically untested as a cohesive unit.

## Options for Next Steps

### Option A: Validate "Planning" Output (System 2 Focus)
Currently, the Executive stub asks the LLM "How should I respond?" and accepts unstructured text.

*   **The Concept**: Update the Executive stub to demand a **structured plan** (JSON) from the LLM instead of free text.
*   **Goal**: Validate the "Planning" aspect of the system. Prove the Executive can reason about *actions* (e.g., `{"tool": "thermostat", "action": "set", "value": 72}`) rather than just generating chat.
*   **Pros**:
    *   Essential for future Actuator integration (moving beyond "chatbot" status).
    *   Makes the "Action" part of the heuristic `{"condition": ..., "action": ...}` concrete rather than abstract text.
*   **Cons**:
    *   Doesn't strictly validate the *learning* mechanism (the "next time is faster" loop).

### Option B: Validate Heuristic Quality (System 1 Focus)
The current `test_heuristic_flow` assumes the LLM outputs a "good" heuristic when asked. We haven't verified if those heuristics are actually matchable.

*   **The Concept**: Run the extraction prompt against 5-10 varied scenarios and rigorously inspect the output.
*   **Goal**: Ensure the "Condition" text generated by the LLM is semantic enough for the embedding model to match later, and not too specific (overfitting) or too vague (hallucinating).
*   **Pros**:
    *   Ensures the "knowledge" being stored is actually usable.
    *   Prevents filling the database with garbage patterns that never fire.
*   **Cons**:
    *   Largely manual/qualitative process initially.

### Option C: Close the Loop (Integration Focus)
Connect the pipes to run a full cycle.

*   **The Concept**: Create a single test case that runs: Event A -> LLM Reasoning -> Feedback -> Store Heuristic -> **Send Event A' (similar)** -> Verify Fast Path.
*   **Goal**: Prove the system *learned* from interaction 1 and applied it to interaction 2.
*   **Pros**:
    *   Definitive proof of the architecture's core value proposition ("The Killer Feature").
*   **Cons**:
    *   High complexity to debug. Failure could be in the LLM, the embedding generation, the threshold tuning, or the storage layer.

## Analysis & Recommendation

Given the complexity of the learning behavior, **Option C** (Integration) carries a high risk of "debugging everything at once."

**Option B** (Quality) is a prudent prerequisite. If the LLM generates unmatchable heuristics (e.g., conditions that are too verbose or hallucinated), Option C will fail regardless of the wiring.

**Option A** (Planning) is valuable but orthogonal to the *learning* mechanism itself. It refines *what* is executed, not *how* it is learned.

**Recommendation**: Start with **Option B** to ensure our "System 2" (LLM) is producing valid inputs for our "System 1" (Heuristics). Once we trust the inputs, we can proceed to **Option C** to prove the loop.
