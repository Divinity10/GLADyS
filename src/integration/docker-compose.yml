# GLADyS Integration Test - Full Stack PoC
# Run: docker-compose up -d
#
# PORTS: Docker uses offset ports to avoid conflict with local development:
#   - postgres: PostgreSQL with pgvector (port 5433)
#   - memory-python: Python Memory Storage (port 50061, not 50051)
#   - memory-rust: Rust SalienceGateway (port 50062, not 50052)
#   - orchestrator: Python Orchestrator (port 50060, not 50050)
#   - executive-stub: Python Executive with optional LLM (port 50063, not 50053)
#
# This allows local services (50050-50053) and Docker (50060-50063) to run simultaneously.
#
# Architecture:
#   - Python handles storage: PostgreSQL CRUD, embedding generation, text search
#   - Rust handles salience: LRU cache, queries Python on cache miss
#   - No bulk loading - heuristics fetched on demand via QueryMatchingHeuristics RPC
#
# Development workflow:
#   - Python code changes: Auto-reload (source mounted as volumes)
#   - Rust code changes: Run 'make rust-rebuild' to rebuild container
#
# LLM configuration (optional):
#   - Set OLLAMA_URL env var to enable LLM responses in Executive
#   - Example: OLLAMA_URL=http://192.168.1.100:11434 docker-compose up -d
#
# Test flow:
#   1. Client sends event to Orchestrator (50050)
#   2. Orchestrator queries SalienceGateway (Rust fast path)
#   3. Orchestrator routes based on salience score
#   4. High salience → immediate to Executive (+ LLM if configured)
#   5. Low salience → accumulate into moment, send on tick (100ms)

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: gladys-integration-db
    environment:
      POSTGRES_USER: gladys
      POSTGRES_PASSWORD: gladys
      POSTGRES_DB: gladys
    ports:
      - "5433:5432"
    volumes:
      - ../memory/migrations:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gladys"]
      interval: 5s
      timeout: 5s
      retries: 5

  memory-python:
    build:
      context: ../memory/python
      dockerfile: Dockerfile
    container_name: gladys-integration-memory-python
    environment:
      STORAGE_HOST: postgres
      STORAGE_PORT: 5432
      STORAGE_USER: gladys
      STORAGE_PASSWORD: gladys
      STORAGE_DATABASE: gladys
    ports:
      - "50061:50051"  # Host 50061 -> Container 50051 (avoids conflict with local)
    # DEV: Mount source so code changes don't require rebuild
    volumes:
      - ../memory/python/gladys_memory:/app/gladys_memory:ro
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.channel_ready_future(grpc.insecure_channel('localhost:50051')).result(timeout=5)"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  memory-rust:
    build:
      context: ../memory
      dockerfile: rust/Dockerfile
    container_name: gladys-integration-memory-rust
    environment:
      RUST_LOG: info
      # Rust fast path can call Python for storage operations
      STORAGE_ADDRESS: http://memory-python:50051
    ports:
      - "50062:50052"  # Host 50062 -> Container 50052 (avoids conflict with local)
    depends_on:
      memory-python:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "50052"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  orchestrator:
    build:
      context: ../orchestrator
      dockerfile: Dockerfile
    container_name: gladys-integration-orchestrator
    environment:
      # Switch between Python (50051) and Rust (50052) here:
      # SALIENCE_MEMORY_ADDRESS: memory-python:50051  # Python path
      SALIENCE_MEMORY_ADDRESS: memory-rust:50052      # Rust fast path
      MEMORY_STORAGE_ADDRESS: memory-python:50051     # For event storage
      EXECUTIVE_ADDRESS: executive-stub:50053
    ports:
      - "50060:50050"  # Host 50060 -> Container 50050 (avoids conflict with local)
    # DEV: Mount source so code changes don't require rebuild
    volumes:
      - ../orchestrator/gladys_orchestrator:/app/gladys_orchestrator:ro
    depends_on:
      memory-rust:
        condition: service_healthy
      executive-stub:
        condition: service_healthy

  # Executive stub (Python) for integration testing
  # The real Executive will be C#/.NET
  #
  # LLM Configuration:
  #   Set OLLAMA_URL to your Ollama server (e.g., http://host.docker.internal:11434)
  #   Set OLLAMA_MODEL to the model name (default: gemma:2b)
  #
  # Memory Configuration:
  #   MEMORY_ADDRESS: Address of Memory service for heuristic storage
  #   Default: memory-python:50051 (uses Python path for storage operations)
  executive-stub:
    build:
      context: ..
      dockerfile: executive/Dockerfile
    container_name: gladys-integration-executive-stub
    environment:
      # Memory service address for storing learned heuristics
      MEMORY_ADDRESS: memory-python:50051
      # Uncomment and set to your Ollama server URL to enable LLM responses
      # OLLAMA_URL: http://host.docker.internal:11434
      # OLLAMA_MODEL: gemma:2b
      OLLAMA_URL: ${OLLAMA_URL:-}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma:2b}
    ports:
      - "50063:50053"  # Host 50063 -> Container 50053 (avoids conflict with local)
    depends_on:
      memory-python:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.channel_ready_future(grpc.insecure_channel('localhost:50053')).result(timeout=5)"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
